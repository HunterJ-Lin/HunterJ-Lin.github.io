<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
    <meta name="description" content="">
    <link href="css/tlzdoc.css" rel="stylesheet" type="text/css">
    <title>Jianghang Lin 林将航's Homepage</title>
    <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>

<body>
<div id="layout-content" style="margin-top: 25px;">
    <table cellspacing="10px">
        <tr>
            <td><img src="images/avatar_white.jpg" alt="linjianghang pic" style="vertical-align:left;width:180px;height:240px;"
                    title="linjianghang"></td>
            <td style="width: 100%;">
                <h3>Jianghang Lin | 林将航</h3>
                <br>
                <a href="http://mac.xmu.edu.cn" target="_blank">Media Analytics and Computing (MAC) Lab</a>, Xiamen University, Xiamen, China.
                <br>
                Emails: hunterjlin007 AT gmail.com (best), hunterjlin007 AT stu.xmu.edu.cn
                <br>
                [<a href="https://scholar.google.com.tw/citations?hl=zh-CN&user=hxZ-5AkAAAAJ" target="_blank">Goolge Scholar</a>]&nbsp;&nbsp;[<a
                    href="https://github.com/HunterJ-Lin" target="_blank">GitHub</a>]&nbsp;&nbsp;[<a href="" target="_blank">Curriculum Vitae</a>] (Last updated on 31 July, 2025)
            </td>
            <td>
                <div style="text-align:center;">
                    <a href="https://clustrmaps.com/site/1bqgx"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=A8QwCDiWay-hstO1YhBC_Oe4V7W8ygYJi8GYY58fdiM&cl=ffffff" /></a>
                    <a href="https://info.flagcounter.com/nOqi"><img src="https://s11.flagcounter.com/count2/nOqi/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
                </div>
            </td>
        </tr>
    </table>
<div id="layout-content" style="margin-top: 25px;">
<h4>[<a style=" color:#9D849A;" href="#biography">Biography</a>] [<a style=" color:#9D849A;" href="#publications">Publications</a>] [<a style=" color:#9D849A;" href="#honors_and_awards">Honors and Awards</a>] </h4>

<h2>Biography<a name="biography"></a>&nbsp;&nbsp;&nbsp;</h2>
    <p style="text-indent:2em;"> I am currently a Ph.D student in <a href="https://en.xmu.edu.cn/" target="_blank">Xiamen University</a>, advised by Prof. <a href="https://mac.xmu.edu.cn/ljcao/" target="_blank">Liujuan Cao</a> and Prof. <a href="https://mac.xmu.edu.cn/rrji_en/" target="_blank">Rongrong Ji</a>. My research interests are in Computer Vision, Multimodal and Machine Learning. Recently, I focus on Open Vocabulary Learning and Large Visual-Language Model.</p>
    <ul>
        <li> 09/2023 -- Now: Ph.D in Computer Science and Technology, <a href="https://en.xmu.edu.cn/" target="_blank">Xiamen University</a>, Xiamen, China </li>
        <li> 05/2025 -- Now: Research Intern, Medical multimodal large model of ByteDance Soaring Star Talent Program, <a href="https://www.bytedance.com/en/" target="_blank">ByteDance</a>, Shanghai, China </li>
        <li> 01/2024 -- 05/2024: Research Intern, <a href="https://www.catl.com/en/" target="_blank">CATL</a>, Ningde, China </li>
        <li> 09/2020 -- 07/2023: M.S. in Pattern Recognition and Intelligent System, <a href="https://en.xmu.edu.cn/" target="_blank">Xiamen University</a>, Xiamen, China </li>
        <li> 12/2022 -- 05/2023: Research Intern, <a href="https://www.catl.com/en/" target="_blank">CATL</a>, Ningde, China </li>
        <li> 02/2022 -- 08/2022: Research Intern under the supervision of Dr. <a href="https://shenyunhang.github.io">Yunhang Shen</a>, <a href="https://open.youtu.qq.com/#/open" target="_blank">Tencent Youtu Lab</a>, Shanghai, China </li>
        <li> 07/2020 -- 08/2020: Development Intern, <a href="https://www.ruijienetworks.com/" target="_blank">Ruijie Networks</a>, Fuzhou, China </li>
        <li> 09/2016 -- 07/2020: B.S. in Software Engineering, <a href="https://en.fzu.edu.cn/" target="_blank">Fuzhou University</a>, Fuzhou, China </li>
    </ul>

<h2>Publications<a name="publications"></a>&nbsp;&nbsp;&nbsp;</h2>
    <h3>Conference</h3>
    <table class="pub_table">
      <tbody>
          <tr>
            <td class="pub_td1"> <img src="images/ActiveTeacher.png" class="papericon"></td>
            <td class="pub_td2"> Peng Mi<sup>*</sup>, <font color="goldenrod">Jianghang Lin<sup>*</sup></font>, Yiyi Zhou<sup>*</sup>, Yunhang Shen, Gen Luo, Xiaoshuai Sun, Liujuan Cao<sup>✉</sup>, Rongrong Fu, Qiang Xu, Rongrong Ji
              <br><b>Active Teacher for Semi-Supervised Object Detection</b>
              <br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022
              <br>
              [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mi_Active_Teacher_for_Semi-Supervised_Object_Detection_CVPR_2022_paper.pdf" target="_blank">pdf</a>]
              [<a href="https://github.com/HunterJ-Lin/ActiveTeacher" target="_blank">code</a>]
              (<sup>*</sup>Equal Contribution)
            </td>
          </tr>
          <tr>
            <td class="pub_td1"> <img src="images/WSOVOD.png" class="papericon"></td>
            <td class="pub_td2"> <font color="goldenrod">Jianghang Lin</font>, Yunhang Shen, Bingquan Wang, Shaohui Lin, Ke Li, Liujuan Cao<sup>✉</sup>
              <br><b>Weakly Supervised Open-Vocabulary Object Detection</b>
              <br>The 38th Annual AAAI Conference on Artificial Intelligence (AAAI), 2024
              <br>
              [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28127/28257" target="_blank">pdf</a>]
              [<a href="https://github.com/HunterJ-Lin/WSOVOD" target="_blank">code</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"> <img src="images/ASRIS.jpg" class="papericon"></td>
            <td class="pub_td2"> Pengfei Yue<sup>*</sup>, <font color="goldenrod">Jianghang Lin<sup>*</sup></font>, Shengchuan Zhang<sup>✉</sup>, Jie Hu, Yilin Lu, Hongwei Niu, Haixin Ding, Yan Zhang, Guannan Jiang, Liujuan Cao, Rongrong Ji
              <br><b>Adaptive Selection based Referring lmage Segmentation</b>
              <br>Proceedings of the 32st ACM International Conference on Multimedia (ACM MM), 2024
              <br>
              [<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3680850" target="_blank">pdf</a>]
              [<a href="https://github.com/swagger-coder/ASDA" target="_blank">code</a>]
              (<sup>*</sup>Equal Contribution)
            </td>
          </tr>
          <tr>
            <td class="pub_td1"> <img src="images/GOI.png" class="papericon"></td>
            <td class="pub_td2"> Yansong Qu<sup>*</sup>, Shaohui Dai<sup>*</sup>, Xinyang Li, <font color="goldenrod">Jianghang Lin</font>, Liujuan Cao<sup>✉</sup>, Shengchuan Zhang, Rongrong Ji
              <br><b>GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary Semantic-space Hyperplane</b>
              <br>Proceedings of the 32st ACM International Conference on Multimedia (ACM MM), 2024
              <br>
              [<a href="https://arxiv.org/pdf/2405.17596" target="_blank">pdf</a>]
              [<a href="https://github.com/Quyans/GOI-Hyperplane" target="_blank">code</a>]
              (<sup>*</sup>Equal Contribution)
            </td>
          </tr>
          <tr>
            <td class="pub_td1"> <img src="images/EOV-Seg.png" class="papericon"></td>
            <td class="pub_td2"> Hongwei Niu<sup>*</sup>, Jie Hu<sup>*</sup>, <font color="goldenrod">Jianghang Lin<sup>*</sup></font>, Guannan Jiang, Shengchuan Zhang<sup>✉</sup>
              <br><b>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation</b>
              <br>The 39th Annual AAAI Conference on Artificial Intelligence (AAAI), 2025
              <br>
              [<a href="https://arxiv.org/abs/2412.08628" target="_blank">pdf</a>]
              [<a href="https://github.com/nhw649/EOV-Seg" target="_blank">code</a>]
              (<sup>*</sup>Equal Contribution)
            </td>
          </tr>
          <tr>
            <td class="pub_td1"> <img src="images/SCSD.png" class="papericon"></td>
            <td class="pub_td2"> Hongwei Niu<sup>*</sup>, Linhuang Xie<sup>*</sup>, <font color="goldenrod">Jianghang Lin<sup>*</sup></font>, Shengchuan Zhang<sup>✉</sup>
              <br><b>Exploring Semantic Consistency and Style Diversity for Domain Generalized Semantic Segmentation</b>
              <br>The 39th Annual AAAI Conference on Artificial Intelligence (AAAI), 2025
              <br>
              [<a href="https://arxiv.org/abs/2412.12050" target="_blank">pdf</a>]
              [<a href="https://github.com/nhw649/SCSD" target="_blank">code</a>]
              (<sup>*</sup>Equal Contribution)
            </td>
          </tr>
          <tr>
            <td class="pub_td1"> <img src="images/U-SAM.png" class="papericon"></td>
            <td class="pub_td2"> Xiaofeng Jin, Jie Hu, <font color="goldenrod">Jianghang Lin</font>, Shengchuan Zhang<sup>✉</sup>, Liujuan Cao
              <br><b>U-SAM: Upgrade Segment Anything Model With Semantic-Aware and Memory-Efficient</b>
              <br> IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025
              <br>
              [<a href="https://ieeexplore.ieee.org/abstract/document/10889270" target="_blank">pdf</a>]
              [<a href="" target="_blank">code</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"> <img src="images/WhatYouPerceiveIsWhatYouConceive.png" class="papericon"></td>
            <td class="pub_td2">  <font color="goldenrod">Jianghang Lin</font>, Yue Hu, Jiangtao Shen, Yunhang Shen, Liujuan Cao<sup>✉</sup>, Shengchuan Zhang, Rongrong Ji
              <br><b>What You Perceive Is What You Conceive: A Cognition-Inspired Framework for Open Vocabulary Image Segmentation</b>
              <br> Proceedings of the 33st ACM International Conference on Multimedia (ACM MM), 2025
              <br>
              [<a href="https://arxiv.org/pdf/2505.19569" target="_blank">pdf</a>]
              [<a href="" target="_blank">code</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"> <img src="images/GAA.jpg" class="papericon"></td>
            <td class="pub_td2">  Yilin Lu<sup>*</sup>, <font color="goldenrod">Jianghang Lin<sup>*</sup></font>, Linhuang Xie, Kai Zhao, Yansong Qu, Shengchuan Zhang<sup>✉</sup>, Liujuan Cao, Rongrong Ji
              <br><b>Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection</b>
              <br> Proceedings of the 33st ACM International Conference on Multimedia (ACM MM), 2025
              <br>
              [<a href="https://arxiv.org/pdf/2507.09619" target="_blank">pdf</a>]
              [<a href="" target="_blank">code</a>]
              (<sup>*</sup>Equal Contribution)
            </td>
          </tr>
      </tbody>
    </table>
<!-- ------------------------------------------------------------------------------------------------------ -->
    <!-- <h3>Journal</h3>
    <table class="pub_table">
        <tbody>     
            <tr>
                <td class="pub_td1"> <img src="./imgs/TNNLS18_MDAL.png" class="papericon"></td>
                <td class="pub_td2">Shengchuan Zhang, Rongrong Ji<sup>✉</sup>, <font color="goldenrod">Jie Hu</font>, Xiaoqiang Lu, Xuelong Li
                <br><b>Face Sketch Synthesis by Multidomain Adversarial Learning</b>
                <br>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2018
                <br>
                [<a href="https://ieeexplore.ieee.org/abstract/document/8478205" target="_blank">pdf</a>]	 
                [<a href="https://github.com/hujiecpp/MDAL" target="_blank">code</a>]                    
                </td>
            </tr>
        </tbody>
    </table> -->
<!-- ------------------------------------------------------------------------------------------------------ -->
    <h3>Preprint</h3>
    <table class="pub_table">
        <tbody>
            <tr>
                <td class="pub_td1"> <img src="images/HUWSOD.png" class="papericon"></td>
                <td class="pub_td2">Liujuan Cao, <font color="goldenrod">Jianghang Lin</font>, Zebo Hong, Yunhang Shen<sup>✉</sup>, Shaohui Lin, Chao Chen, Rongrong Ji
                <br><b>HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection</b>
                <br>arXiv preprint arXiv:2406.19394, 2024
                <br>
                [<a href="https://arxiv.org/pdf/2406.19394" target="_blank">arXiv</a>]
                [<a href="https://github.com/shenyunhang/HUWSOD" target="_blank">code</a>]
                </td>
            </tr>   	
            <tr>
                <td class="pub_td1"> <img src="images/DGSSIS.png" class="papericon"></td>
                <td class="pub_td2">Xin Chen, Jie Hu, Xiawu Zheng, <font color="goldenrod">Jianghang Lin</font>, Liujuan Cao<sup>✉</sup>, Rongrong Ji
                <br><b>Depth-Guided Semi-Supervised Instance Segmentation</b>
                <br>arXiv preprint arXiv:2406.17413, 2024
                <br>
                [<a href="https://arxiv.org/pdf/2406.17413" target="_blank">arXiv</a>]
                [<a href="" target="_blank">code</a>]
                </td>
            </tr>     
        </tbody>
    </table>

<h2>Honors and Awards<a name="honors_and_awards"></a>&nbsp;&nbsp;&nbsp;</h2>
    <ul>
        <li>  Second Prize of The 6th "China Software Cup" Software Design Competition for College Students, 2017  </li>
        <li>  Second Prize of The 8th National College Students' service outsourcing entrepreneurship and innovation competition (P.R.China), 2017  </li> 
    </ul>
</div>
</div>
<center>
    <div id="clustrmaps-widget" style="width:20%">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=A8QwCDiWay-hstO1YhBC_Oe4V7W8ygYJi8GYY58fdiM"></script>
        <br>
        <p>© Jianghang Lin 林将航</p>
  </div>
    
</center>
</body>
